# One Piece Wiki Scraper Update

## Goal
Update the current One Piece Wiki scraper so that when given a main article title, it:
1. Fetches the main article content via the MediaWiki API.
2. Detects all related sub-articles (e.g., `Monkey D. Luffy/History`, `Monkey D. Luffy/Powers_and_Abilities`).
3. Fetches each sub-article via the API.
4. Merges them into the final output in order.

## Requirements
- Use the [One Piece Wiki API](https://onepiece.fandom.com/api.php).
- Keep everything in one Python file except for `requirements.txt`.
- Maintain the scraper’s existing functionality for the main article.
- Ensure sub-article fetching happens automatically without manual list creation.

## Implementation Steps
1. **Fetch Main Article**  
   Use:
   ```http
   GET https://onepiece.fandom.com/api.php?action=parse&page=<ARTICLE_TITLE>&prop=text&format=json
Extract Sub-Articles

Parse the returned HTML text for <a> tags with href starting with /wiki/<ARTICLE_TITLE>/.

Example match: /wiki/Monkey_D._Luffy/History

Normalize the link into a valid API page parameter (replace spaces with underscores, decode URL entities).

Fetch Sub-Articles
For each sub-article:

http
Copy
Edit
GET https://onepiece.fandom.com/api.php?action=parse&page=<SUB_ARTICLE_TITLE>&prop=text&format=json
Merge Content

Append sub-article HTML/text to the main article’s content in the order they appear in the link list.

Avoid duplicates.

Ensure the output is clean and readable.

Output

Save the merged article to the target folder.

Folder name = main article title.

File name = content.html (or .txt if plain text is used).

Notes
Use requests for HTTP calls.

Use BeautifulSoup for HTML parsing.

Ensure robust handling of URL encoding/decoding.

If pllimit is exceeded, handle pagination in the API response.

pgsql
Copy
Edit

If you want, I can now **write the updated Python code** exactly following these `.mdc` steps so Cur